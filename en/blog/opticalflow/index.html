<!doctype html><html itemscope lang=en-us itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5"><meta name=theme-name content="hugoplate"><link rel="shortcut icon" href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_96x0_resize_lanczos_3.png type=image/x-icon><link rel=icon href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_96x0_resize_lanczos_3.png type=image/x-icon><link rel=icon type=image/png sizes=48x48 href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_48x0_resize_lanczos_3.png><link rel=icon type=image/png sizes=96x96 href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_96x0_resize_lanczos_3.png><link rel=apple-touch-icon sizes=144x144 href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_144x0_resize_lanczos_3.png><link rel=manifest href=/manifest.webmanifest><meta name=msapplication-TileColor content="#ddd"><meta name=theme-color content="#ffffff"><base href=https://lhfgghc.github.io/en/blog/opticalflow/><title>【学习笔记】Optical Flow的基本原理与常见光流估计方法</title>
<meta name=keywords content="Boilerplate,Hugo"><meta name=description content="this is meta description"><meta name=author content="zeon.studio"><meta property="og:image" content="https://lhfgghc.github.io/images/opticalflow/image-3.png"><meta name=twitter:image content="https://lhfgghc.github.io/images/opticalflow/image-3.png"><meta name=twitter:card content="summary_large_image"><meta property="og:image:width" content="1341"><meta property="og:image:height" content="700"><meta property="og:image:type" content="image/
        .png
      "><meta property="og:title" content="【学习笔记】Optical Flow的基本原理与常见光流估计方法"><meta property="og:description" content="this is meta description"><meta property="og:type" content="website"><meta property="og:url" content="https://lhfgghc.github.io/en/blog/opticalflow/"><meta name=twitter:title content="【学习笔记】Optical Flow的基本原理与常见光流估计方法"><meta name=twitter:description content="this is meta description"><script>let indexURL="https://lhfgghc.github.io/en/searchindex.json",includeSectionsInSearch=["blog"],search_no_results="No results for",search_initial_message="Type something to search.."</script><meta http-equiv=x-dns-prefetch-control content="on"><link rel=preconnect href=https://use.fontawesome.com crossorigin><link rel=preconnect href=//cdnjs.cloudflare.com><link rel=preconnect href=//www.googletagmanager.com><link rel=preconnect href=//www.google-analytics.com><link rel=dns-prefetch href=https://use.fontawesome.com><link rel=dns-prefetch href=//ajax.googleapis.com><link rel=dns-prefetch href=//cdnjs.cloudflare.com><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=dns-prefetch href=//www.google-analytics.com><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//connect.facebook.net><link rel=dns-prefetch href=//platform.linkedin.com><link rel=dns-prefetch href=//platform.twitter.com><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Heebo:wght@400;600&family=Signika:wght@500;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><link href="/css/style.min.fca510e246f4fc84c347f1be4e9577ee9ef74e47c120609233d7fd9e5c9f9aa2.css" integrity="sha256-/KUQ4kb0/ITDR/G+TpV37p73TkfBIGCSM9f9nlyfmqI=" rel=stylesheet><link defer async rel=stylesheet href="/css/style-lazy.min.ec14d2549e8e0fbaf9af20dcc88fad37fb352e5bd5978a801dc1c3f5a5922174.css" integrity="sha256-7BTSVJ6OD7r5ryDcyI+tN/s1LlvVl4qAHcHD9aWSIXQ=" media=print onload='this.media="all",this.onload=null'></head><body><header class="header sticky top-0 z-30"><nav class="navbar container"><div class=order-0><a class="navbar-brand block" href=/en/><img fetchpriority=high decoding=async class="img logo-light" width=160 height=32 src=/images/logo_hud3822dc52499c854acb9b180fed4f736_3648_320x0_resize_q80_h2_lanczos_3.webp alt=Hugoplate onerror='this.onerror=null,this.src="/images/logo_hud3822dc52499c854acb9b180fed4f736_3648_320x0_resize_lanczos_3.png"'>
<img fetchpriority=high decoding=async class="img logo-dark" width=160 height=32 src=/images/logo-darkmode_hu95dd250582672ebe0c063cf60eed448f_3090_320x0_resize_q80_h2_lanczos_3.webp alt=Hugoplate onerror='this.onerror=null,this.src="/images/logo-darkmode_hu95dd250582672ebe0c063cf60eed448f_3090_320x0_resize_lanczos_3.png"'></a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Menu Open</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Menu Close</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8"><li class=nav-item><a class=nav-link href=/en/>主页</a></li><li class=nav-item><a class=nav-link href=/en/about/>个人信息</a></li><li class=nav-item><a class=nav-link href=/en/blog/>学习笔记</a></li><li class="nav-item nav-dropdown group relative"><span class="nav-link
inline-flex items-center">Pages<svg class="h-4 w-4 fill-current" viewBox="0 0 20 20"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z"/></svg></span><ul class="nav-dropdown-list lg:group-hover:visible lg:group-hover:opacity-100"><li class=nav-dropdown-item><a class=nav-dropdown-link href=/en/404.html>404 Page</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/en/authors/>Authors</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/en/blog/>Blog</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/en/categories/>Categories</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/en/contact/>Contact</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/en/elements/>Elements</a></li></ul></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="border-border text-dark hover:text-primary dark:border-darkmode-border mr-5 inline-block border-r pr-5 text-xl dark:text-white dark:hover:text-darkmode-primary" data-target=search-modal>
<i class="fa-solid fa-search"></i></button><div class="theme-switcher mr-5"><input id=theme-switcher data-theme-switcher type=checkbox>
<label for=theme-switcher><span class=sr-only>theme switcher</span>
<span><svg class="absolute left-1/2 top-1/2 -translate-x-1/2 -translate-y-1/2 z-10 opacity-100 dark:opacity-0" viewBox="0 0 56 56" fill="#fff" height="16" width="16"><path d="M30 4.6c0-1-.9-2-2-2a2 2 0 00-2 2v5c0 1 .9 2 2 2s2-1 2-2zm9.6 9a2 2 0 000 2.8c.8.8 2 .8 2.9.0L46 13a2 2 0 000-2.9 2 2 0 00-3 0zm-26 2.8c.7.8 2 .8 2.8.0.8-.7.8-2 0-2.9L13 10c-.7-.7-2-.8-2.9.0-.7.8-.7 2.1.0 3zM28 16A12 12 0 0016 28a12 12 0 0012 12 12 12 0 0012-12A12 12 0 0028 16zm23.3 14c1.1.0 2-.9 2-2s-.9-2-2-2h-4.9a2 2 0 00-2 2c0 1.1 1 2 2 2zM4.7 26a2 2 0 00-2 2c0 1.1.9 2 2 2h4.9c1 0 2-.9 2-2s-1-2-2-2zm37.8 13.6a2 2 0 00-3 0 2 2 0 000 2.9l3.6 3.5a2 2 0 002.9.0c.8-.8.8-2.1.0-3zM10 43.1a2 2 0 000 2.9c.8.7 2.1.8 3 0l3.4-3.5c.8-.8.8-2.1.0-2.9s-2-.8-2.9.0zm20 3.4c0-1.1-.9-2-2-2a2 2 0 00-2 2v4.9c0 1 .9 2 2 2s2-1 2-2z"/></svg><svg class="absolute left-1/2 top-1/2 -translate-x-1/2 -translate-y-1/2 z-10 opacity-0 dark:opacity-100" viewBox="0 0 24 24" fill="none" height="16" width="16"><path fill="#000" fill-rule="evenodd" clip-rule="evenodd" d="M8.2 2.2c1-.4 2 .6 1.6 1.5-1 3-.4 6.4 1.8 8.7a8.4 8.4.0 008.7 1.8c1-.3 2 .5 1.5 1.5v.1A10.3 10.3.0 0112.4 22 10.3 10.3.0 013.2 6.7c1-2 2.9-3.5 4.9-4.4z"/></svg></span></label></div><script>var darkMode=!1,themeSwitch;window.matchMedia("(prefers-color-scheme: dark)").matches&&(darkMode=!0),localStorage.getItem("theme")==="dark"?darkMode=!0:localStorage.getItem("theme")==="light"&&(darkMode=!1),darkMode&&document.documentElement.classList.toggle("dark"),themeSwitch=document.querySelectorAll("[data-theme-switcher]"),document.addEventListener("DOMContentLoaded",()=>{[].forEach.call(themeSwitch,function(e){e.checked=!!darkMode,e.addEventListener("click",()=>{document.documentElement.classList.toggle("dark"),localStorage.setItem("theme",document.documentElement.classList.contains("dark")?"dark":"light")})})})</script></div></nav></header><div class=search-modal aria-hidden=true style=--color-primary:#121212><div data-target=close-search-modal class=search-modal-overlay></div><div class=search-wrapper data-image=true data-description=true data-tags=true data-categories=true><div class=search-wrapper-header><label for=search-modal-input style=margin-top:-1px><span class=sr-only>search icon</span>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" height="18" width="18" class="search-icon" data-type="search"><path fill="currentcolor" d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" height="18" width="18" class="search-reset" data-type="reset"><path fill="currentcolor" d="M256 512A256 256 0 10256 0a256 256 0 100 512zM175 175c9.4-9.4 24.6-9.4 33.9.0l47 47 47-47c9.4-9.4 24.6-9.4 33.9.0s9.4 24.6.0 33.9l-47 47 47 47c9.4 9.4 9.4 24.6.0 33.9s-24.6 9.4-33.9.0l-47-47-47 47c-9.4 9.4-24.6 9.4-33.9.0s-9.4-24.6.0-33.9l47-47-47-47c-9.4-9.4-9.4-24.6.0-33.9z"/></svg>
</label><input id=search-modal-input type=text data-search-input autocomplete=off aria-label=Search placeholder="Search Post ..."></div><div class=search-wrapper-body><div class=search-result data-search-result></div><span class=search-result-empty>Type something to search..</span></div><div class=search-wrapper-footer><span><kbd><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="currentcolor" viewBox="0 0 16 16"><path d="M3.204 11h9.592L8 5.519 3.204 11zm-.753-.659 4.796-5.48a1 1 0 011.506.0l4.796 5.48c.566.647.106 1.659-.753 1.659H3.204a1 1 0 01-.753-1.659z"/></svg>
</kbd><kbd><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="currentcolor" style="margin-top:1px" viewBox="0 0 16 16"><path d="M3.204 5h9.592L8 10.481 3.204 5zm-.753.659 4.796 5.48a1 1 0 001.506.0l4.796-5.48c.566-.647.106-1.659-.753-1.659H3.204a1 1 0 00-.753 1.659z"/></svg>
</kbd>to navigate
</span><span><kbd><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentcolor" style="display:inline-block" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M14.5 1.5a.5.5.0 01.5.5v4.8a2.5 2.5.0 01-2.5 2.5H2.707l3.347 3.346a.5.5.0 01-.708.708l-4.2-4.2a.5.5.0 010-.708l4-4a.5.5.0 11.708.708L2.707 8.3H12.5A1.5 1.5.0 0014 6.8V2a.5.5.0 01.5-.5z"/></svg>
</kbd>to select
</span><span class=search-result-info></span>
<span data-target=close-search-modal><kbd>ESC</kbd> to close</span></div></div></div><main><section class="section pt-7"><div class=container><div class="row justify-center"><article class=lg:col-10><div class=mb-10><picture><source srcset=/images/opticalflow/image-3_hue7cf7ea89128ba28bd2370291193be06_375826_545x0_resize_q80_h2_lanczos_3.webp media="(max-width: 575px)"><source srcset=/images/opticalflow/image-3_hue7cf7ea89128ba28bd2370291193be06_375826_600x0_resize_q80_h2_lanczos_3.webp media="(max-width: 767px)"><source srcset=/images/opticalflow/image-3_hue7cf7ea89128ba28bd2370291193be06_375826_700x0_resize_q80_h2_lanczos_3.webp media="(max-width: 991px)"><source srcset=/images/opticalflow/image-3_hue7cf7ea89128ba28bd2370291193be06_375826_1110x0_resize_q80_h2_lanczos_3.webp><img loading=lazy decoding=async src=/images/opticalflow/image-3_hue7cf7ea89128ba28bd2370291193be06_375826_1110x0_resize_lanczos_3.png class="w-full rounded img" alt="【学习笔记】Optical Flow的基本原理与常见光流估计方法" width=1341 height=700></picture></div><h1 class="h2 mb-4">【学习笔记】Optical Flow的基本原理与常见光流估计方法</h1><ul class=mb-4><li class="mr-4 inline-block"><a href=/en/authors/liang-hangfeng/><i class="fa-regular fa-circle-user mr-2"></i>Liang Hangfeng</a></li><li class="mr-4 inline-block"><i class="fa-regular fa-folder mr-2"></i>
<a href=/en/categories/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0/>深度学习
,
</a><a href=/en/categories/%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/>学习笔记</a></li><li class="mr-4 inline-block"><i class="fa-regular fa-clock mr-2"></i>
March 2, 2024</li></ul><div class="content mb-10"><p>CSDN：<a href="https://blog.csdn.net/m0_59701064/article/details/136310029?spm=1001.2014.3001.5501" target=_blank>【学习笔记】光流(Optical Flow)的基本知识与光流估计方法</a></p><h3 id=1-简介>1. 简介</h3><h4 id=11-光流optical-flow的基本概念>1.1 光流(Optical Flow)的基本概念</h4><p>光流（Optical Flow）是指描述在图像序列中物体表面上的像素移动的模式。在计算机视觉和图像处理领域，光流是一种用于估计相邻帧之间像素之间位移的技术。简而言之，<strong>光流是描述相邻图像帧之间像素运动的模式，它通过跟踪图像中的特征点或像素来估计运动的速度和方向。</strong> 或者说是在像素级别上的位移描述，即每一个像素点从第一帧到第二帧的具体偏移量。</p><h4 id=12-补充说明>1.2 补充说明</h4><p><strong>光流法的基本假设</strong>：（1）亮度恒定不变，即同一目标在不同帧间的运动时亮度不会发生改变；（2）相邻帧之间的位移要比较小，即时间上不会引起目标的剧烈位置变化。</p><p><strong>基于假设的形式化表达</strong>：形式化表达即如下，其中I代表具体的像素值（光强度），t为具体的时刻，Δt一般即为相邻帧的时间，(u,v)则为像素的具体偏移量，可以理解成前一帧图像中的(i,j)像素在Δt的时间间隔下（下一帧），移动了(u,v)。</p><p>$$ I(i,j,t) = I(i+u,j+v,t+\Delta t) $$</p><p><strong>光流的方向</strong>：图像帧A→B，则flow的方向为B->A。（不确定，查阅了各种资料后得到的解释，但这部分方向上的定义不影响实际的分析与代码编写）</p><p><strong>光流的warp</strong>：若存在两帧frame1和frame2，不考虑batch的情况下，其shape均为[3,H,W]，对应的flow的shape为[2,H,W]，两个通道分别表示水平、竖直方向上的位移，该flow满足以下关系式子。
$$
frame1[:,i,j] = frame2[:,i+u,j+v] \
u=flow[0,i,j],v=flow[1,i,j]
$$
那么在已知frame1和flow的情况下，可以通过依次查询flow并将像素进行偏移得到frame2，该过程则为warp。在大多数代码场景下，还会涉及到<strong>backwarp</strong>操作（有些文章不严格区分warp和backwarp），即在已知frame2和flow的情况下，得到frame1，一部分原因还包括backwarp的操作可以通过<strong>torch.nn.functional.grid_sample</strong>采样函数实现（见后文），这样的话，我们可以在已知frame2到frame1的flow以及frame1的前提下轻松地求得frame2。（Tips：部分文章提及使用frame1到frame2的-flow求frame2，使用的还是基于grid_sample函数的backwarp方法，个人认为并不正确，因为frame2的像素点和具体在frame1的采样点的映射关系需要通过flow得以确定，画个图就可以说明。）</p><h4 id=13-常见应用>1.3 常见应用</h4><p>视频压缩、视频插帧、物体运动分析（车流方向）、特征计算的优化、视频稳像、视觉效果等。</p><h3 id=2-一些常用操作>2. 一些常用操作</h3><h4 id=21-光流文件读取>2.1 光流文件读取</h4><p>flow的常见存储文件格式为.flo，网络上已有现成且广泛使用的flo文件读取代码，如下。通过分析代码可以知道该文件大致的存储结构，包括标识符、高宽数据、具体位移数据。通过open函数进行二进制形式打开即可，使用numpy中的fromfile函数进行依次读取。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#75715e># 读取flow文件，返回对应的numpy数组</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_flow</span>(path):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(path,<span style=color:#e6db74>&#39;rb&#39;</span>) <span style=color:#66d9ef>as</span> fl:
</span></span><span style=display:flex><span>        flag <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>fromfile(fl,np<span style=color:#f92672>.</span>float32,count<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#75715e># 先读取第一个数据</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> (<span style=color:#ae81ff>202021.25</span> <span style=color:#f92672>==</span> flag) <span style=color:#75715e># 使用assert语句判定flo文件的合法性</span>
</span></span><span style=display:flex><span>        width <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>fromfile(fl, np<span style=color:#f92672>.</span>int32, count<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)[<span style=color:#ae81ff>0</span>] <span style=color:#75715e># 读取宽、高信息</span>
</span></span><span style=display:flex><span>        height <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>fromfile(fl, np<span style=color:#f92672>.</span>int32, count<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>fromfile(fl, np<span style=color:#f92672>.</span>float32, count<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> width <span style=color:#f92672>*</span> height) <span style=color:#75715e># 读取剩余的位移数据</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>resize(data, (height, width, <span style=color:#ae81ff>2</span>))
</span></span></code></pre></div><h4 id=22-光流可视化>2.2 光流可视化</h4><p>将运动信息进行具体的可视化。其实就是对不同位移方向和大小进行颜色赋值，例如绿色代表往右上角进行移动，绿色越深表示偏移程度越大。</p><p>例如可以达到以下的效果。（实际位移方向与颜色之间的映射关系可能不同，下图中左上角的椅子往右上角移动了，颜色为深紫色。）</p><p><img alt=image-1 src=/images/opticalflow/image-1.png></p><p>网上常用的可视化代码如下，可以直接当成已知模块使用。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 将flow进行可视化</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>flow_to_image</span>(flow, max_flow<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 确保max_flow的最小值为1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> max_flow <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        max_flow <span style=color:#f92672>=</span> max(max_flow, <span style=color:#ae81ff>1.</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        max_flow <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>max(flow)
</span></span><span style=display:flex><span>	<span style=color:#75715e># 定值，含义未深究，可能是用于得到不同的饱和度</span>
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 提取水平、垂直运动信息</span>
</span></span><span style=display:flex><span>    u, v <span style=color:#f92672>=</span> flow[:, :, <span style=color:#ae81ff>0</span>], flow[:, :, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#75715e># 计算每个像素点的光流大小，即为模长（位移大小）</span>
</span></span><span style=display:flex><span>    mag <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sqrt(np<span style=color:#f92672>.</span>square(u) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>square(v))
</span></span><span style=display:flex><span>    <span style=color:#75715e># 计算每个像素点的光流方向，即速度的方向角度（得到的弧度）</span>
</span></span><span style=display:flex><span>    angle <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arctan2(v, u)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 将角度值映射到0到1的范围内，除以2*pi，加1并余1即可约束范围（色调信息）</span>
</span></span><span style=display:flex><span>    im_h <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mod(angle <span style=color:#f92672>/</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>pi) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 大概是：计算每个像素点的饱和度，通过光流大小和max_flow的比值进行缩放，并确保饱和度在0到1的范围内</span>
</span></span><span style=display:flex><span>    im_s <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>clip(mag <span style=color:#f92672>*</span> n <span style=color:#f92672>/</span> max_flow, a_min<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, a_max<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 大概是：计算每个像素点的亮度，亮度为n减去饱和度，并确保亮度在0到1的范围内</span>
</span></span><span style=display:flex><span>    im_v <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>clip(n <span style=color:#f92672>-</span> im_s, a_min<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, a_max<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 将色调、饱和度和亮度转换为RGB颜色空间的图像</span>
</span></span><span style=display:flex><span>    im <span style=color:#f92672>=</span> hsv_to_rgb(np<span style=color:#f92672>.</span>stack([im_h, im_s, im_v], <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>    <span style=color:#75715e># 在0到255的范围内表示图像的像素值</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (im <span style=color:#f92672>*</span> <span style=color:#ae81ff>255</span>)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>uint8)
</span></span></code></pre></div><h4 id=23-backwarp操作实现>2.3 backwarp操作实现</h4><p>对于正向warp，其含义比较简单，即将每一个像素点数值放到对应的偏移位置即可，注意非整点的处理。</p><p>这里重点介绍一下使用torch.nn.functional.grid_sample的backwarp操作，由于backwarp操作一般在网络内部执行，这里写的是torch框架下的代码。</p><p>对于grid_sample函数，网上资料较多，不做赘述，简单来说就是输入一个图像和一个网格grid，根据网格grid中的位置信息在图像中找到（采样）对应的像素点进行存储，看后续代码应该更好理解。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#75715e># 这里把它封装成了一个nn模块，将backwarp操作写到了正向传播过程中，输入frame2与flow，得到对应的frame1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BackWarpFlow</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 类静态变量，其实可以写成全局变量。由于后面的操作有需要处理指定规格的网格，有时候无需重复处理以减少时间开销，这里先做一个存储</span>
</span></span><span style=display:flex><span>    Warp_Grid <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(BackWarpFlow, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, img, flow):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将运算加载到GPU上，如果传入的张量已在GPU上加载则可以不用操作</span>
</span></span><span style=display:flex><span>        img, flow <span style=color:#f92672>=</span> img<span style=color:#f92672>.</span>cuda(), flow<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>        <span style=color:#75715e># 若该规格的flow未被处理过（未生成过对应规格的网格）则先处理出一个网格</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> str(flow<span style=color:#f92672>.</span>shape) <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> BackWarpFlow<span style=color:#f92672>.</span>Warp_Grid:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 使用linspace生成一个从-1.0到1.0的等间隔的张量（与flow的宽等长），并调整规格为[1,1,1,W]</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 使用repeat函数在shape[2]方向上重复shape[2]次，即H次，得到张量的规格为[1,1,H,W]</span>
</span></span><span style=display:flex><span>            u <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>, flow<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>3</span>])<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>repeat(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, flow<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>], <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            <span style=color:#75715e># 同理另一层</span>
</span></span><span style=display:flex><span>            v <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>, flow<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>])<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>repeat(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, flow<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>3</span>])
</span></span><span style=display:flex><span>            <span style=color:#75715e># 在第1个维度上拼接得到的网格规格为[1,2,H,W]，与flow相对应，且每一个像素位置上的值是均匀的，与其位置相对应。存储到字典中                 </span>
</span></span><span style=display:flex><span>            BackWarpFlow<span style=color:#f92672>.</span>Warp_Grid[str(flow<span style=color:#f92672>.</span>shape)] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([u, v], <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>        <span style=color:#75715e># 取出flow中两个方向上的偏移，将其根据规格进行规范化处理，重新拼接</span>
</span></span><span style=display:flex><span>        tenFlow <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([flow[:, <span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>1</span>, :, :] <span style=color:#f92672>*</span> (<span style=color:#ae81ff>2.0</span> <span style=color:#f92672>/</span> (img<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>3</span>] <span style=color:#f92672>-</span> <span style=color:#ae81ff>1.0</span>)),
</span></span><span style=display:flex><span>                             flow[:, <span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>2</span>, :, :] <span style=color:#f92672>*</span> (<span style=color:#ae81ff>2.0</span> <span style=color:#f92672>/</span> (img<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>-</span> <span style=color:#ae81ff>1.0</span>))], <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 调用grid_sample进行采样，前面提到Warp_Grid中的网格值是与位置一一对应的，加上flow后，每个网格上的值就相当于进行了位移，将该网格作为采样网格即可实现frame2还原到frame1。（理解成，(i,j)地方的像素应当去(i+u,j+v)的地方采样，(i,j)即在Warp_Grid中体现，与位置对应，(u,v)即在tenFlow体现）</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># permute调整维度以适应grid_sample函数，该函数要求通道数这一维在最后</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 可能有非整点的情况出现，采用双线性插值。</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>grid_sample(input<span style=color:#f92672>=</span>img,
</span></span><span style=display:flex><span>                                               grid<span style=color:#f92672>=</span>(BackWarpFlow<span style=color:#f92672>.</span>Warp_Grid[str(tenFlow<span style=color:#f92672>.</span>shape)] <span style=color:#f92672>+</span> tenFlow)<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>                                               mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bilinear&#39;</span>,
</span></span><span style=display:flex><span>                                               padding_mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;border&#39;</span>,
</span></span><span style=display:flex><span>                                               align_corners<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><h4 id=24-光流的评价loss说明>2.4 光流的评价(Loss说明)</h4><p>最主要的评价指标为Endpoint error (EPE) loss，即端点误差损失。EPE损失是通过计算预测的光流场与真实的光流场之间的平均欧氏距离来衡量光流估计的精度。同时也可以作为损失函数进行相关光流估计网络的训练。该部分代码实现方式较多，这里呈现最简单的实现方法。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EPELoss</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(EPELoss, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, flow_pred, flow_target):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 平均欧氏距离</span>
</span></span><span style=display:flex><span>        dist <span style=color:#f92672>=</span> (flow_target <span style=color:#f92672>-</span> flow_pred)<span style=color:#f92672>.</span>pow(<span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>sqrt()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> dist<span style=color:#f92672>.</span>mean()
</span></span></code></pre></div><h3 id=3-光流估计方法>3. 光流估计方法</h3><p>前人有许多基于数理统计、泰勒展开等数学方法，内容较多难以涵盖，主要介绍归纳几种常见有效的基于深度学习的光流估计方法（当然是经过论文阅读和学习的方法，没有学习到的方法若后续有所学习，则会补上笔记）。大部分方法本文只能给出整体的实现思路，具体的开源代码分析耗时较长，后续再进行整理编辑。同时，对于每种方法的创新点和亮点，不同读者可能有不同的理解看法，本文视角可能相对局限。</p><h4 id=31-flownet-基于cnns构建的含关联层的端到端训练光流网络>3.1 FlowNet-基于CNNs构建的含关联层的端到端训练光流网络</h4><h5 id=311-论文及其链接>3.1.1 论文及其链接</h5><p><strong>论文</strong>：ICCV2015, FlowNet: Learning Optical Flow with Convolutional Networks</p><p><strong>链接</strong>：<a href=https://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf target=_blank>FlowNet: Learning Optical Flow With Convolutional Networks (thecvf.com)</a></p><h5 id=312-文章摘要>3.1.2 文章摘要</h5><p>卷积神经网络（CNNs）最近在各种计算机视觉任务中取得了很大成功，尤其是与识别相关的任务。光流估计并不是CNNs成功的任务之一。在本文中，我们构建了能够将光流估计问题作为监督学习任务解决的CNNs。我们提出并比较了两种架构：一种<strong>通用架构</strong>和另一种<strong>包括在不同图像位置相关特征向量的层</strong>。由于现有的地面真实数据集不足以训练CNN，我们生成了一个大型的合成<strong>飞行椅子</strong>（Flying Chairs）数据集。我们展示了在这种不真实的数据上训练的网络仍然很好地推广到现有数据集，如Sintel和KITTI，在5到10fps的帧速率下达到了竞争性的准确度。</p><h5 id=313-文章亮点>3.1.3 文章亮点</h5><ol><li>提出了使用卷积神经网络来进行光流估计的方法，例如一种通用架构，说明了使用CNNs方法进行光流估计的可行性；</li><li>提出了像素关联层（correlation layer）的概念，再像素级别描述<strong>各像素之间的关联性</strong>，使用网络自行学习两张图像（或其特征）之间的关联性，以实现图像在不同位置之间的匹配；</li><li>在多个尺度上不断细化光流（Refinement），对低分辨率的粗糙进行上采样并逐步学习细化；</li><li>制作了Flying Chairs数据集，提供了一种含噪声背景情况下的物体运动信息（Flow）的数据集。</li></ol><h5 id=314-网络结构>3.1.4 网络结构</h5><p>文章中包含了两种网络结构，一种为简单版本的FlowNetSimple，另一种是基于像素关联层的FlowNetCorr，结构分别如下。</p><p><img alt=image-2 src=/images/opticalflow/image-2.png></p><p><strong>Refinement模块</strong></p><p>两类网络均包含了refinement细化模块，该模块以多尺度的图像特征为输入，得到细化的光流估计（提高光流的输出分辨率），结构如下。（但从效果来说，又比双线性插值好多少呢）</p><p><img alt=image-3 src=/images/opticalflow/image-3.png></p><p>可以看到，灰色部分即为编码器（特征提取部分）所得到的多尺度特征。该细化模块进行了多个尺度的粗糙光流细化，具体地，相当于将以下三部分内容进行拼接：上一层特征经过卷积与上卷积后的结果（绿色）、上一层输出的粗糙flow的上采样结果（红色）、编码器过程中对应尺度的特征（灰色），作为当前层的特征，送入一个光流预测网络即可获得当前层输出的光流。</p><p><strong>FlowNetSimple</strong></p><p>简单地将两张图像进行拼接(在通道维度上进行拼接)，并使用卷积神经网络进行多尺度的特征提取，最后送入细化模块进行光流的细化。该网络结构是由网络自行决定如何从图像中进行运动信息的提取。对输出Flow进行监督达到光流估计的目的。</p><p><strong>FlowNetCorr</strong></p><p>根据网络结构可知对两帧图像分别进行特征的采集，随后对特征进行关联性计算，得到一个特征相关性矩阵（暂且这么叫吧）。</p><p>首先文章中给出了两个特征图中<strong>任意两个像素之间的关联性</strong>定义，如下表达式。
$$
c\left(\mathbf{x}<em>{1}, \mathbf{x}</em>{2}\right)=\sum_{\mathbf{o} \in[-k, k] \times[-k, k]}\left\langle\mathbf{f}<em>{1}\left(\mathbf{x}</em>{1}+\mathbf{o}\right), \mathbf{f}<em>{2}\left(\mathbf{x}</em>{2}+\mathbf{o}\right)\right\rangle
$$
上式含义为：以x1这个点为中心，2k+1为边长的正方形区域，与x2为中心的区域之间的关联性，这里引入一个k作为区域范围，以说明一个个patch块之间的关联性（其实就是考虑到了周边的像素点的特征）。其中x1和x2分别表示来自于两个特征图的像素位置，分别都是二元组；f1表示第一个特征图，f2表示第二个特征图，由于特征图是多通道的（如前面的网络结构图中显示的两个计算相关性的特征图，均有256层特征），因此每一个像素位置都可以取到多通道的特征数值（例如在第一个特征图的(i,j)位置，可以取到一个256维的向量feature[: , : , i , j])，将两个区域块对应位置的特征向量进行内积运算，得到一个相关值c。</p><p>但每个特征图都有h×w个像素，两张特征图中两两求关联性则会产生h×h×w×w个相关值，这个数量太大了，因此文章中提到对于<strong>每个位置的像素仅考虑一个限定范围D之内的像素的相关性</strong>，即不考虑处理距离太远的像素之间的相关项，这样便将值相关性数据的数理降低为h×w×D×D。即可以理解为：对于第一张图的每一个位置的像素特征，都对应着和第二张图D×D个相关性值。这也是图中441这个数值的来源，这里的D应取为21，即考虑以某一像素为中心，边长为21的范围内的像素的相关性。</p><p>此外，观察网络结构图可以发现，文章还将该相关项矩阵与第一张图的再采样的特征图（以尺寸为1的卷积核获取了32层特征）进行了拼接操作，这一步文章中并未详细说明（可能也是我没读仔细），感性理解上可能是，后续的网络学习需要同时带上第一帧图像的一些特征信息，进行特征融合。</p><p>通过相关性矩阵的处理，网络能够学习到像素级别上的像素相关性信息，从而更好地捕获像素级别的运动信息，在学习到抽象的相关性特征的同时不丢失浅层的图像信息。</p><h4 id=32-spynet-基于空间金字塔与残差光流训练的光流更新网络>3.2 SPyNet-基于空间金字塔与残差光流训练的光流更新网络</h4><h5 id=321-论文及其链接>3.2.1 论文及其链接</h5><p><strong>论文</strong>：CVPR2017, Optical Flow Estimation using a Spatial Pyramid Network</p><p><strong>链接</strong>：<a href=https://openaccess.thecvf.com/content_cvpr_2017/papers/Ranjan_Optical_Flow_Estimation_CVPR_2017_paper.pdf target=_blank>Optical Flow Estimation Using a Spatial Pyramid Network (thecvf.com)</a></p><h5 id=322-文章摘要>3.2.2 文章摘要</h5><p>我们通过将<strong>经典的空间金字塔形式与深度学习相结合</strong>来学习计算光流。这通过在每个金字塔级别将一对图像中的一个图像根据当前的光流估计进行warp，并计算对光流的更新来<strong>以粗到细的方式</strong>估计大运动。与在每个金字塔级别进行标准目标函数最小化的方法不同，我们针对<strong>每个级别训练一个深度网络</strong>来计算光流的<strong>更新</strong>。与最近的FlowNet方法不同，网络不需要处理大运动；这些大运动由金字塔来处理。这有几个优点。首先，我们的空间金字塔网络（SPyNet）在模型参数方面比FlowNet简单得多，小96%。这使得它更高效，更适用于嵌入式应用。其次，由于每个金字塔级别的光流很小（&lt;1像素），因此将卷积方法应用于一对变形图像是合适的。第三，与FlowNet不同，学习到的卷积滤波器与经典的时空滤波器相似，这提供了对方法及其改进方法的见解。我们的结果比大多数标准基准测试上的FlowNet更精确，这表明了将经典的光流方法与深度学习相结合的新方向。</p><h5 id=323-文章亮点>3.2.3 文章亮点</h5><ol><li>采用训练光流残差的方式，对光流进行逐步细化；</li><li>每一个尺度下均训练了一个深度网络用于光流残差的获取，并有多尺度下的光流细化过程；</li><li>将传统的空间金字塔与深度学习方法进行结合，同时参数量较小。</li></ol><h5 id=324-网络结构>3.2.4 网络结构</h5><p>SPyNet的网络结构如下，其中仅举例展示了三层金字塔结构，具体的金字塔层数可以根据实际情况进行修改。</p><p><img alt=image-4 src=/images/opticalflow/image-4.png></p><p>其中可以明显看到每一层的网络结构大致相同，故这里取其中一层的结构进行说明，如红框所示，首先需要有上一层网络产出的预测光流（需要通过u上采样达到目标分辨率，或者双线性插值）。对于深度网络G，输入深度网络的有三部分内容：上一层的预测光流，frame1，经过上一层预测光流backwarp的frame2；而输出深度网络的则为光流残差，需要与上一层的预测光流进行相加以实现这一尺度下的光流细化。</p><p>在最开始的情况下，直接暴力地设置初始光流为0，通过多尺度的空间特征与光流细化后得到目标尺度的光流。对于具体的公式表达，文章说明的比较详细，但最终表达的意思与网络结构图一致。</p><p><strong>模型训练</strong></p><p>根据文章描述，是对每一层的深度网络G进行依次训练，并不是一次性的端到端训练，其中Loss的定义如下图，即在前面的所有层的网络完成训练后，认定可以产出较为精准的光流，该层深度网络训练得到光流残差$v_k$后，与当前尺度下的光流标签$\hat V_k$和上一层预测光流的差$\hat v_k$进行Loss计算，使用EPELoss。</p><p><img alt=image-5 src=/images/opticalflow/image-5.png></p><p>但为什么不能直接对加上光流残差修正后的光流进行监督呢？含义似乎是一样的。</p><p>这样分层训练的方式简化了模型的训练，相比于端到端的训练方法来说更省时间或容易收敛。（不知道能不能对多层光流同时进行监督？）</p><h5 id=325-代码分析>3.2.5 代码分析</h5><p>开源代码中大部分训练代码给的为lua代码，但也给出了使用Pytorch框架编写的网络结构，为更好地理解网络结构，对代码进行分析（lua没学过）。</p><p>去除一些辅助代码，这里只看核心的网络架构。取自开源的Pytorch代码（https://github.com/sniklaus/pytorch-spynet），进行注释编写分析。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Network</span>(torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>		
</span></span><span style=display:flex><span>        <span style=color:#75715e># 为更好地训练模型，编写了一个图像归一化处理的过程代码</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Preprocess</span>(torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>                super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>			
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, tenInput):
</span></span><span style=display:flex><span>                <span style=color:#75715e># 翻转通道，为什么呢？</span>
</span></span><span style=display:flex><span>                tenInput <span style=color:#f92672>=</span> tenInput<span style=color:#f92672>.</span>flip([<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>                <span style=color:#75715e># 减去数据集的均值并除以标准差以实现图像的归一化</span>
</span></span><span style=display:flex><span>                tenInput <span style=color:#f92672>=</span> tenInput <span style=color:#f92672>-</span> torch<span style=color:#f92672>.</span>tensor(data<span style=color:#f92672>=</span>[<span style=color:#ae81ff>0.485</span>, <span style=color:#ae81ff>0.456</span>, <span style=color:#ae81ff>0.406</span>], dtype<span style=color:#f92672>=</span>tenInput<span style=color:#f92672>.</span>dtype, device<span style=color:#f92672>=</span>tenInput<span style=color:#f92672>.</span>device)<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>                tenInput <span style=color:#f92672>=</span> tenInput <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>tensor(data<span style=color:#f92672>=</span>[<span style=color:#ae81ff>1.0</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>0.229</span>, <span style=color:#ae81ff>1.0</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>0.224</span>, <span style=color:#ae81ff>1.0</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>0.225</span>], dtype<span style=color:#f92672>=</span>tenInput<span style=color:#f92672>.</span>dtype, device<span style=color:#f92672>=</span>tenInput<span style=color:#f92672>.</span>device)<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> tenInput
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>		<span style=color:#75715e># 每层的光流残差训练的深度网络</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Basic</span>(torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>def</span> __init__(self, intLevel):
</span></span><span style=display:flex><span>                super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>				<span style=color:#75715e># 与论文实验部分说明一致，采用五个卷积层加ReLU激活函数</span>
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>netBasic <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>                    <span style=color:#75715e># 说明为什么输入通道为8。</span>
</span></span><span style=display:flex><span>                    <span style=color:#75715e># 前面提及送入网络的三部分内容为：上一层的预测光流（2通道），frame1（3通道），经过上一层预测光流backwarp的frame2（3通道）</span>
</span></span><span style=display:flex><span>                    <span style=color:#75715e># 直接拼接融合即为8通道</span>
</span></span><span style=display:flex><span>                    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>),
</span></span><span style=display:flex><span>                    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),
</span></span><span style=display:flex><span>                    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>),
</span></span><span style=display:flex><span>                    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),
</span></span><span style=display:flex><span>                    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>),
</span></span><span style=display:flex><span>                    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),
</span></span><span style=display:flex><span>                    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>),
</span></span><span style=display:flex><span>                    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),
</span></span><span style=display:flex><span>                    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>			
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, tenInput):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>netBasic(tenInput)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>netPreprocess <span style=color:#f92672>=</span> Preprocess()
</span></span><span style=display:flex><span>		
</span></span><span style=display:flex><span>        <span style=color:#75715e># 每一层都有一个深度网络用于训练当前尺度的光流残差</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>netBasic <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>ModuleList([ Basic(intLevel) <span style=color:#66d9ef>for</span> intLevel <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>6</span>) ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, tenOne, tenTwo):
</span></span><span style=display:flex><span>        tenFlow <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>		
</span></span><span style=display:flex><span>        <span style=color:#75715e># 用于存储不同尺度下的图像金字塔 开始就是原尺度</span>
</span></span><span style=display:flex><span>        tenOne <span style=color:#f92672>=</span> [ self<span style=color:#f92672>.</span>netPreprocess(tenOne) ]
</span></span><span style=display:flex><span>        tenTwo <span style=color:#f92672>=</span> [ self<span style=color:#f92672>.</span>netPreprocess(tenTwo) ]
</span></span><span style=display:flex><span>		
</span></span><span style=display:flex><span>        <span style=color:#75715e># 额外处理五层</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> intLevel <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>5</span>):
</span></span><span style=display:flex><span>            <span style=color:#75715e># 取金字塔顶部（最小的）图像，若宽高大于32则可以继续下采样</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> tenOne[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>32</span> <span style=color:#f92672>or</span> tenOne[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>3</span>] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>32</span>:
</span></span><span style=display:flex><span>                <span style=color:#75715e># 使用均值池化获取低分辨率的图像， insert到list的第0位，则金字塔中的尺度是自小向大的</span>
</span></span><span style=display:flex><span>                tenOne<span style=color:#f92672>.</span>insert(<span style=color:#ae81ff>0</span>, torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>avg_pool2d(input<span style=color:#f92672>=</span>tenOne[<span style=color:#ae81ff>0</span>], kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, count_include_pad<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>))
</span></span><span style=display:flex><span>                tenTwo<span style=color:#f92672>.</span>insert(<span style=color:#ae81ff>0</span>, torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>avg_pool2d(input<span style=color:#f92672>=</span>tenTwo[<span style=color:#ae81ff>0</span>], kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, count_include_pad<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>))
</span></span><span style=display:flex><span>		
</span></span><span style=display:flex><span>        <span style=color:#75715e># 开始光流为0，shape除以2是因为每一层的操作都有一个光流上采样过程，为了同步编码，第0层也进行一个上采样，因此设置小一点的初始光流</span>
</span></span><span style=display:flex><span>        tenFlow <span style=color:#f92672>=</span> tenOne[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>new_zeros([ tenOne[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], <span style=color:#ae81ff>2</span>, int(math<span style=color:#f92672>.</span>floor(tenOne[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>/</span> <span style=color:#ae81ff>2.0</span>)), int(math<span style=color:#f92672>.</span>floor(tenOne[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>3</span>] <span style=color:#f92672>/</span> <span style=color:#ae81ff>2.0</span>)) ])
</span></span><span style=display:flex><span>		
</span></span><span style=display:flex><span>        <span style=color:#75715e># 遍历每一层</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> intLevel <span style=color:#f92672>in</span> range(len(tenOne)):
</span></span><span style=display:flex><span>            <span style=color:#75715e># 对光流进行上采样，方式为双线性插值，缩放比为2，</span>
</span></span><span style=display:flex><span>            tenUpsampled <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>interpolate(input<span style=color:#f92672>=</span>tenFlow, scale_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bilinear&#39;</span>, align_corners<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#f92672>*</span> <span style=color:#ae81ff>2.0</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 对上采样的光流尺寸进行修正，处理可能存在的尺寸不匹配问题，需要与金字塔图片的尺度进行匹配。</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> tenUpsampled<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>!=</span> tenOne[intLevel]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>]: tenUpsampled <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>pad(input<span style=color:#f92672>=</span>tenUpsampled, pad<span style=color:#f92672>=</span>[ <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span> ], mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;replicate&#39;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> tenUpsampled<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>3</span>] <span style=color:#f92672>!=</span> tenOne[intLevel]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>3</span>]: tenUpsampled <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>pad(input<span style=color:#f92672>=</span>tenUpsampled, pad<span style=color:#f92672>=</span>[ <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span> ], mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;replicate&#39;</span>)
</span></span><span style=display:flex><span>			
</span></span><span style=display:flex><span>            <span style=color:#75715e># 将前面提及的三部分送入深度网络训练，得到光流残差最后加上上采样后的前一层的预测光流</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># tenOne[intLevel]  第一帧</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># backwarp(tenInput=tenTwo[intLevel], tenFlow=tenUpsampled) 通过当前光流backwarp后的第二帧</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># tenUpsampled 当前预测光流</span>
</span></span><span style=display:flex><span>            tenFlow <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>netBasic[intLevel](torch<span style=color:#f92672>.</span>cat([ tenOne[intLevel], backwarp(tenInput<span style=color:#f92672>=</span>tenTwo[intLevel], tenFlow<span style=color:#f92672>=</span>tenUpsampled), tenUpsampled ], <span style=color:#ae81ff>1</span>)) <span style=color:#f92672>+</span> tenUpsampled
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> tenFlow
</span></span></code></pre></div><h4 id=33-raft-基于多尺度4d相关体查找与gru的迭代光流更新网络>3.3 RAFT-基于多尺度4D相关体查找与GRU的迭代光流更新网络</h4><h5 id=331-论文及其链接>3.3.1 论文及其链接</h5><p><strong>论文</strong>：ECCV2020, Recurrent All-Pairs Field Transforms for Optical Flow</p><p><strong>链接</strong>：<a href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470392.pdf target=_blank>ecva.net/papers/eccv_2020/papers_ECCV/papers/123470392.pdf</a></p><h5 id=332-文章摘要>3.3.2 文章摘要</h5><p>我们介绍了循环全对场变换（RAFT），这是一种用于光流的新型深度网络架构。RAFT提取<strong>每个像素的特征</strong>，为所有像素对构建<strong>多尺度4D相关性体</strong>，并通过一个<strong>循环单元迭代</strong>地更新流场，该单元在相关性体上<strong>执行查找</strong>操作。RAFT实现了最先进的性能。在KITTI数据集上，RAFT实现了5.10%的F1-all误差，比最佳发布结果（6.10%）减少了16%的误差。在Sintel（最终通行证）上，RAFT的端点误差为2.855像素，比最佳发布结果（4.098像素）减少了30%的误差。此外，RAFT在推理时间、训练速度和参数数量方面具有高效率，具有强大的跨数据集泛化能力。</p><h5 id=333-文章亮点>3.3.3 文章亮点</h5><ol><li>采用多尺度的4D相关体，即相关体金字塔，通过预处理该金字塔结合后续的查表操作，可以描述一个flow所带来的映射像素的多尺度相关性</li><li>采用GRU对光流进行不断的修正与更新。</li></ol><h5 id=334-网络结构>3.3.4 网络结构</h5><p>RAFT的整体结构如下，后文进行分模块分析。</p><p><img alt=image-6 src=/images/opticalflow/image-6.png></p><p><strong>特征提取网络</strong></p><p>即上图中的Feature Encoder和Context Encoder，分别进行图像特征和语义特征的提取。其中Feature Encoder导出低分辨率的密集特征图。（对于特征提取无非是卷积操作，这里也不深究特征提取到什么程度了，到后面看看对特征图进行怎样的处理）</p><p><strong>4D相关体（4D Correlation Volumes）计算</strong></p><p>看结构图可以看到一个点积的符号，联想到对特征向量的点积操作，这里和FlowNet中的关联矩阵计算很类似，只是这里只考虑两个像素之间的特征相关性，并没有选定某一个范围进行区域相关性计算。公式如下，本质上就是将两个像素（比如第一张图的(i,j)和第二张图的(k,l)）对应的特征进行点积得到一个特征值，那么像素两两进行相关性计算即可得到一个4维（H×W×H×W）的相关矩阵，通称为4D相关体，<strong>表示$I_1$每一个像素点与每一个$I_2$像素点的相关度</strong>。
$$
C_{ijkl}=\sum_{h}g_{\theta}(\mathbf{I_1})<em>{ijh}·g</em>{\theta}(\mathbf{I_2})_{klh}
$$
<strong>相关性金字塔（Correlation Pyramid）</strong></p><p>但实际上，frame1（前文说是$I_1$）的像素不可能与frame2的每一个像素都有关联（可能还有其它更深层的考虑），作者再4D相关体的后两个维度上进行下采样，得到了4层的相关体，其中尺寸从大到小分别为H×W×H×W、H×W×H/2×W/2、H×W×H/4×W/4、H×W×H/8×W/8，这里也在一定程度上隐含了像素和某一个区域的相关性。如下图所示。</p><p><img alt=image-7 src=/images/opticalflow/image-7.png></p><p><strong>查表Lookup</strong></p><p>这一部分论文写的相对难以理解，不过可以去读源码辅助理解。</p><p>由于RAFT主要进行的是光流的迭代更新，对于每一次产出的光流，可以表示为（$\mathbf{f}^{1},\mathbf{f}^{2}$），对于frame1中的一个像素点$\mathbf{x}(u,v)$，通过光流找到frame2中的对应点$\mathbf{x&rsquo;}(u+\mathbf{f}^{1}(u),v+\mathbf{f}^{2}(v))$，在$\mathbf{x&rsquo;}$的基础上，作者定义了一个领域网格$N_r(\mathbf{x&rsquo;})$，表达式如下：
$$
\mathbf{N_r(x&rsquo;)} = { \mathbf{x&rsquo;}+\mathbf{dx}\ |\ \mathbf{dx}\in \mathbb{Z}^2, ||\mathbf{dx}||_1 \le r }
$$
相当于这个领域网格就是以当前预测光流的映射关系下，找到对应像素点的一个领域范围内，并在这个领域范围内去查找相关性金字塔，将查询得到的值进行组合得到一个新的向量，该向量就是Lookup的输出结果。（不太标准地概括这种思路：对于任意一个flow，在4D相关体上查询frame1的像素在该flow的作用下的映射像素之间的多尺度关联性，关联性越高那么显然这个flow的映射更贴近真实情况，这样便给与网络一种描述flow准确度的手段）</p><p>举个例子，比如r为9，那么对于每一个frame1的像素点，都可以在4D相关性金字塔的每一层中查询到81个相关性值，即为与邻域范围内所有像素点的相关性，而4层相关体则可以查询到4×81个相关性值（对于低像素的相关体，使用双线性采样器找到对应的位置即可），那么对于所有H×W个像素点，最终可以查询到规格[H,W,324]的向量。</p><p>对于上式，r如果较大，那么能够获取到的关联性数据就多，能够获取较大运动范围内的关联信息，反之则运动更为精细。</p><p><strong>光流迭代更新（GRU）</strong></p><p>GRU的大体结构和式子不作赘述，直接看以下网络结构，用于修正光流的产生。</p><p><img alt=image-8 src=/images/opticalflow/image-8.png></p><p>对于每一次迭代，GRU单元的输入（图中红色）包含：经过Lookup查表后的关联向量、隐状态、语义特征context feature；输出（图中蓝色）包含：更新后的隐状态，修正光流Δf，用于对上一次迭代的粗糙光流进行修正。</p><p>特别的是，训练过程中，作者并非直接取最后一次迭代的光流与实际光流计算Loss，而是对每一次迭代的光流都进行了Loss计算以监督。</p><p>这个思路与SPyNet略有点相似（也只是一点，在于使用深度网络训练残差光流，对光流进行细化），只是RAFT在全分辨率下不断进行光流的修正，能够实现每次迭代的监督，而SPyNet则是在多尺度下进行光流的细化。</p><p>以下的内容等待进一步深入学习与笔记的细化。</p><h4 id=34-gma-基于全局运动信息>3.4 GMA-基于全局运动信息</h4><h5 id=341-论文及其链接>3.4.1 论文及其链接</h5><p><strong>论文</strong>：ICCV2021, Learning to Estimate Hidden Motions with Global Motion Aggregation</p><p><strong>链接</strong>：<a href=https://openaccess.thecvf.com//content/ICCV2021/papers/Jiang_Learning_To_Estimate_Hidden_Motions_With_Global_Motion_Aggregation_ICCV_2021_paper.pdf target=_blank>Learning To Estimate Hidden Motions With Global Motion Aggregation (thecvf.com)</a></p><h4 id=35-kpa-基于局部注意力>3.5 KPA-基于局部注意力</h4><h5 id=351-论文及其链接>3.5.1 论文及其链接</h5><p><strong>论文</strong>：CVPR2022，Learning Optical Flow with Kernel Patch Attention</p><p><strong>链接</strong>：<a href=https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_Learning_Optical_Flow_With_Kernel_Patch_Attention_CVPR_2022_paper.pdf target=_blank>Learning Optical Flow With Kernel Patch Attention (thecvf.com)</a></p><h4 id=36-dip-基于patchmatch降低4d相关体的计算>3.6 DIP-基于patchmatch降低4D相关体的计算</h4><h5 id=361-论文及其链接>3.6.1 论文及其链接</h5><p><strong>论文</strong>：CVPR2022, DIP: Deep Inverse Patchmatch for High-Resolution Optical Flow</p><p><strong>链接</strong>：<a href=https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_DIP_Deep_Inverse_Patchmatch_for_High-Resolution_Optical_Flow_CVPR_2022_paper.pdf target=_blank>DIP: Deep Inverse Patchmatch for High-Resolution Optical Flow (thecvf.com)</a></p><h3 id=4-光流的应用研究>4. 光流的应用研究</h3><h4 id=41-iccv2017semantic-video-cnns-through-representation-warping>4.1 ICCV2017：Semantic Video CNNs through Representation Warping</h4><p>使用offline的flow计算方法对前一帧的feature进行warp，但warp的是CNN过程中的层特征。提高精度。</p><p><img alt=image-20240222211502408 src=c:\\Users\\26510\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240222211502408.png></p><h4 id=42-cvpr2018semantic-video-segmentation-by-gated-recurrent-flow-propagation>4.2 CVPR2018：Semantic Video Segmentation by Gated Recurrent Flow Propagation</h4><p>将前一帧的mask进行warp，与当前帧直接计算的mask共同送入GRU，以实现前一帧mask对当前帧mask的弥补。提高精度。</p><p><img alt=image-20240222214105261 src=c:\\Users\\26510\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240222214105261.png></p><h4 id=43-cvpr2017deep-feature-flow-for-video-recognition>4.3 CVPR2017：Deep Feature Flow for Video Recognition</h4><p>将前一帧（key frame）的feature通过flow作为当前帧的feature，减少重复的特征计算。</p><p><img alt=image-20240222214442612 src=c:\\Users\\26510\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240222214442612.png></p><h3 id=参考资料>参考资料</h3><p><a href="https://zhuanlan.zhihu.com/p/446456418?utm_id=0" target=_blank>optical flow光流方向问题 - 知乎 (zhihu.com)</a></p><p><a href=https://blog.csdn.net/qq_37236149/article/details/130207608 target=_blank>光流、warp和grid_sample()_pytorch warp-CSDN博客</a></p><p><a href=https://blog.csdn.net/qq_40390668/article/details/130822631 target=_blank>提取光流文件(.flo)为u，v二通道图像(作为网络输入)_光流flo 文件-CSDN博客</a></p><p><a href=https://zhuanlan.zhihu.com/p/677641957 target=_blank>端点误差损失Endpoint error (EPE) loss - 知乎 (zhihu.com)</a></p><p><a href=https://blog.csdn.net/u010087277/article/details/111593541 target=_blank>【光流评估】《Optical Flow Estimation using a Spatial Pyramid Network》2016 Tubingen, Germany-CSDN博客</a></p><p><a href=https://blog.csdn.net/Yong_Qi2015/article/details/126775274 target=_blank>全面回顾 | 基于深度学习的光流估计算法汇总-CSDN博客</a></p><p><a href=https://www.cnblogs.com/curiositywang/p/16779462.html target=_blank>RAFT光流估计 - CuriosityWang - 博客园 (cnblogs.com)</a></p><p><a href=https://github.com/sniklaus/pytorch-spynet target=_blank>https://github.com/sniklaus/pytorch-spynet</a></p></div><div class="row items-start justify-between"><div class="lg:col-5 mb-10 flex items-center lg:mb-0"><h5 class=mr-3>Tags :</h5><ul><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/en/tags/%e5%85%89%e6%b5%81%e4%bc%b0%e8%ae%a1/>光流估计</a></li><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/en/tags/%e9%87%91%e5%ad%97%e5%a1%94%e7%89%b9%e5%be%81/>金字塔特征</a></li><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/en/tags/gru%e8%bf%ad%e4%bb%a3%e6%9b%b4%e6%96%b0/>Gru迭代更新</a></li></ul></div><div class="lg:col-4 flex items-center"><div class=share-icons><h5 class=share-title>Share :</h5><a class="share-link share-facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flhfgghc.github.io%2fen%2fblog%2fopticalflow%2f" target=_blank rel=noopener aria-label="share facebook"><span class=share-icon><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/></svg>
</span></a><a class="share-link share-twitter" href="https://twitter.com/intent/tweet/?text=Share&amp;url=https%3a%2f%2flhfgghc.github.io%2fen%2fblog%2fopticalflow%2f" target=_blank rel=noopener aria-label="share twitter"><span aria-hidden=true class=share-icon><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8 2H1l8.26 11.015L1.45 22H4.1l6.388-7.349L16 22h7l-8.608-11.478L21.8 2h-2.65l-5.986 6.886zm9 18L5 4h2l12 16z"/></svg>
</span></a><a class="share-link share-email" href="mailto:?subject=Share&amp;body=https%3a%2f%2flhfgghc.github.io%2fen%2fblog%2fopticalflow%2f" target=_self rel=noopener aria-label="share email"><span aria-hidden=true class=share-icon><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 4H2C.9 4 0 4.9.0 6v12c0 1.1.9 2 2 2h20c1.1.0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17.0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1.0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08.0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/></svg>
</span></a><a class="share-link share-reddit" href="https://reddit.com/submit/?url=https%3a%2f%2flhfgghc.github.io%2fen%2fblog%2fopticalflow%2f&amp;resubmit=true&amp;title=Share" target=_blank rel=noopener aria-label="share reddit"><span aria-hidden=true class=share-icon><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M24 11.5c0-1.65-1.35-3-3-3-.96.0-1.86.48-2.42 1.24-1.64-1-3.75-1.64-6.07-1.72.08-1.1.4-3.05 1.52-3.7.72-.4 1.73-.24 3 .5C17.2 6.3 18.46 7.5 20 7.5c1.65.0 3-1.35 3-3s-1.35-3-3-3c-1.38.0-2.54.94-2.88 2.22-1.43-.72-2.64-.8-3.6-.25-1.64.94-1.95 3.47-2 4.55-2.33.08-4.45.7-6.1 1.72C4.86 8.98 3.96 8.5 3 8.5c-1.65.0-3 1.35-3 3 0 1.32.84 2.44 2.05 2.84-.03.22-.05.44-.05.66.0 3.86 4.5 7 10 7s10-3.14 10-7c0-.22-.02-.44-.05-.66 1.2-.4 2.05-1.54 2.05-2.84zM2.3 13.37C1.5 13.07 1 12.35 1 11.5c0-1.1.9-2 2-2 .64.0 1.22.32 1.6.82-1.1.85-1.92 1.9-2.3 3.05zm3.7.13c0-1.1.9-2 2-2s2 .9 2 2-.9 2-2 2-2-.9-2-2zm9.8 4.8c-1.08.63-2.42.96-3.8.96-1.4.0-2.74-.34-3.8-.95-.24-.13-.32-.44-.2-.68.15-.24.46-.32.7-.18 1.83 1.06 4.76 1.06 6.6.0.23-.13.53-.05.67.2.14.23.06.54-.18.67zm.2-2.8c-1.1.0-2-.9-2-2s.9-2 2-2 2 .9 2 2-.9 2-2 2zm5.7-2.13c-.38-1.16-1.2-2.2-2.3-3.05.38-.5.97-.82 1.6-.82 1.1.0 2 .9 2 2 0 .84-.53 1.57-1.3 1.87z"/></svg></span></a></div></div></div><div class=mt-20><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//themefisher-template.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></article></div></div></section></main><footer class="bg-theme-light dark:bg-darkmode-theme-light"><div class=container><div class="row items-center py-10"><div class="lg:col-3 mb-8 text-center lg:mb-0 lg:text-left"><a class="navbar-brand inline-block" href=/en/><img fetchpriority=high decoding=async class="img logo-light" width=160 height=32 src=/images/logo_hud3822dc52499c854acb9b180fed4f736_3648_320x0_resize_q80_h2_lanczos_3.webp alt=Hugoplate onerror='this.onerror=null,this.src="/images/logo_hud3822dc52499c854acb9b180fed4f736_3648_320x0_resize_lanczos_3.png"'>
<img fetchpriority=high decoding=async class="img logo-dark" width=160 height=32 src=/images/logo-darkmode_hu95dd250582672ebe0c063cf60eed448f_3090_320x0_resize_q80_h2_lanczos_3.webp alt=Hugoplate onerror='this.onerror=null,this.src="/images/logo-darkmode_hu95dd250582672ebe0c063cf60eed448f_3090_320x0_resize_lanczos_3.png"'></a></div><div class="lg:col-6 mb-8 text-center lg:mb-0"><ul><li class="m-3 inline-block"><a href=/en/about/>个人信息</a></li><li class="m-3 inline-block"><a href=/en/blog/>学习笔记</a></li><li class="m-3 inline-block"><a href=/en/privacy-policy/>相关说明</a></li></ul></div><div class="lg:col-3 mb-8 text-center lg:mb-0 lg:mt-0 lg:text-right"><ul class=social-icons><li><a target=_blank aria-label=github rel="nofollow noopener" href=https://www.github.com/COOOIKX><i class="fab fa-github"></i></a></li><li><a target=_blank aria-label=csdn rel="nofollow noopener" href="https://blog.csdn.net/m0_59701064?spm=1000.2115.3001.5343"><i class="fas fa-home-lg"></i></a></li></ul></div></div></div><div class="border-border dark:border-darkmode-border border-t py-7"><div class="text-light dark:text-darkmode-light container text-center"><p>HeDesigned & Developed by <a href=https://zeon.studio target=_blank>Zeon Studio</a></p></div></div></footer><script crossorigin=anonymous integrity="sha256-YQerunHGeT7hXzxweSqFUXgOHHxFceSjmMy/kmnAHWU=" src=/js/script.min.6107abba71c6793ee15f3c70792a8551780e1c7c4571e4a398ccbf9269c01d65.js></script><script defer async crossorigin=anonymous integrity="sha256-w+aS42D2+B+Jix+joZ7pAua1vbu/pRK/IhoP55b8n3w=" src=/js/script-lazy.min.c3e692e360f6f81f898b1fa3a19ee902e6b5bdbbbfa512bf221a0fe796fc9f7c.js></script><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/service-worker.js")</script></body></html>