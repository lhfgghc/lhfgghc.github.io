<!doctype html><html itemscope lang=en-us itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5"><meta name=theme-name content="hugoplate"><link rel="shortcut icon" href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_96x0_resize_lanczos_3.png type=image/x-icon><link rel=icon href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_96x0_resize_lanczos_3.png type=image/x-icon><link rel=icon type=image/png sizes=48x48 href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_48x0_resize_lanczos_3.png><link rel=icon type=image/png sizes=96x96 href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_96x0_resize_lanczos_3.png><link rel=apple-touch-icon sizes=144x144 href=/images/favicon_hueb84ecec72665a83aae8c940dfe71474_1906_144x0_resize_lanczos_3.png><link rel=manifest href=/manifest.webmanifest><meta name=msapplication-TileColor content="#ddd"><meta name=theme-color content="#ffffff"><base href=https://lhfgghc.github.io/en/blog/bert/><title>【学习笔记】BERT的基本结构与原理</title>
<meta name=keywords content="Boilerplate,Hugo"><meta name=description content="this is meta description"><meta name=author content="zeon.studio"><meta property="og:image" content="https://lhfgghc.github.io/images/bert/image-2.png"><meta name=twitter:image content="https://lhfgghc.github.io/images/bert/image-2.png"><meta name=twitter:card content="summary_large_image"><meta property="og:image:width" content="1080"><meta property="og:image:height" content="606"><meta property="og:image:type" content="image/
        .png
      "><meta property="og:title" content="【学习笔记】BERT的基本结构与原理"><meta property="og:description" content="this is meta description"><meta property="og:type" content="website"><meta property="og:url" content="https://lhfgghc.github.io/en/blog/bert/"><meta name=twitter:title content="【学习笔记】BERT的基本结构与原理"><meta name=twitter:description content="this is meta description"><script>let indexURL="https://lhfgghc.github.io/en/searchindex.json",includeSectionsInSearch=["blog"],search_no_results="未找到结果",search_initial_message="输入内容以搜索"</script><meta http-equiv=x-dns-prefetch-control content="on"><link rel=preconnect href=https://use.fontawesome.com crossorigin><link rel=preconnect href=//cdnjs.cloudflare.com><link rel=preconnect href=//www.googletagmanager.com><link rel=preconnect href=//www.google-analytics.com><link rel=dns-prefetch href=https://use.fontawesome.com><link rel=dns-prefetch href=//ajax.googleapis.com><link rel=dns-prefetch href=//cdnjs.cloudflare.com><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=dns-prefetch href=//www.google-analytics.com><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//connect.facebook.net><link rel=dns-prefetch href=//platform.linkedin.com><link rel=dns-prefetch href=//platform.twitter.com><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Heebo:wght@400;600&family=Signika:wght@500;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><link href="/css/style.min.4c1d4c3120637c4b33d33d38988a2525a8e15057b5ab0cfa95ce5147d4c95adc.css" integrity="sha256-TB1MMSBjfEsz0z04mIolJajhUFe1qwz6lc5RR9TJWtw=" rel=stylesheet><link defer async rel=stylesheet href="/css/style-lazy.min.ec14d2549e8e0fbaf9af20dcc88fad37fb352e5bd5978a801dc1c3f5a5922174.css" integrity="sha256-7BTSVJ6OD7r5ryDcyI+tN/s1LlvVl4qAHcHD9aWSIXQ=" media=print onload='this.media="all",this.onload=null'></head><body><header class="header sticky top-0 z-30"><nav class="navbar container"><div class=order-0><a class="navbar-brand block" href=/en/><img fetchpriority=high decoding=async class="img logo-light" width=160 height=32 src=/images/logo-1_hubf409f6fc77577ff09f7cacc73653041_8513_320x0_resize_q80_h2_lanczos_3.webp alt=Hugoplate onerror='this.onerror=null,this.src="/images/logo-1_hubf409f6fc77577ff09f7cacc73653041_8513_320x0_resize_lanczos_3.png"'>
<img fetchpriority=high decoding=async class="img logo-dark" width=160 height=32 src=/images/logo-2_hu06a606f114baa5722e40403b02652855_7961_320x0_resize_q80_h2_lanczos_3.webp alt=Hugoplate onerror='this.onerror=null,this.src="/images/logo-2_hu06a606f114baa5722e40403b02652855_7961_320x0_resize_lanczos_3.png"'></a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Menu Open</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Menu Close</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8"><li class=nav-item><a class=nav-link href=/en/>主页</a></li><li class=nav-item><a class=nav-link href=/en/blog/>学习笔记</a></li><li class=nav-item><a class=nav-link href=/en/about/>个人信息</a></li><li class="nav-item nav-dropdown group relative"><span class="nav-link
inline-flex items-center">其它<svg class="h-4 w-4 fill-current" viewBox="0 0 20 20"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z"/></svg></span><ul class="nav-dropdown-list lg:group-hover:visible lg:group-hover:opacity-100"><li class=nav-dropdown-item><a class=nav-dropdown-link href=/en/elements/>样式Draft</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/en/privacy-policy/>相关说明</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/en/categories/>笔记分类</a></li></ul></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="border-border text-dark hover:text-primary dark:border-darkmode-border mr-5 inline-block border-r pr-5 text-xl dark:text-white dark:hover:text-darkmode-primary" data-target=search-modal>
<i class="fa-solid fa-search"></i></button><div class="theme-switcher mr-5"><input id=theme-switcher data-theme-switcher type=checkbox>
<label for=theme-switcher><span class=sr-only>theme switcher</span>
<span><svg class="absolute left-1/2 top-1/2 -translate-x-1/2 -translate-y-1/2 z-10 opacity-100 dark:opacity-0" viewBox="0 0 56 56" fill="#fff" height="16" width="16"><path d="M30 4.6c0-1-.9-2-2-2a2 2 0 00-2 2v5c0 1 .9 2 2 2s2-1 2-2zm9.6 9a2 2 0 000 2.8c.8.8 2 .8 2.9.0L46 13a2 2 0 000-2.9 2 2 0 00-3 0zm-26 2.8c.7.8 2 .8 2.8.0.8-.7.8-2 0-2.9L13 10c-.7-.7-2-.8-2.9.0-.7.8-.7 2.1.0 3zM28 16A12 12 0 0016 28a12 12 0 0012 12 12 12 0 0012-12A12 12 0 0028 16zm23.3 14c1.1.0 2-.9 2-2s-.9-2-2-2h-4.9a2 2 0 00-2 2c0 1.1 1 2 2 2zM4.7 26a2 2 0 00-2 2c0 1.1.9 2 2 2h4.9c1 0 2-.9 2-2s-1-2-2-2zm37.8 13.6a2 2 0 00-3 0 2 2 0 000 2.9l3.6 3.5a2 2 0 002.9.0c.8-.8.8-2.1.0-3zM10 43.1a2 2 0 000 2.9c.8.7 2.1.8 3 0l3.4-3.5c.8-.8.8-2.1.0-2.9s-2-.8-2.9.0zm20 3.4c0-1.1-.9-2-2-2a2 2 0 00-2 2v4.9c0 1 .9 2 2 2s2-1 2-2z"/></svg><svg class="absolute left-1/2 top-1/2 -translate-x-1/2 -translate-y-1/2 z-10 opacity-0 dark:opacity-100" viewBox="0 0 24 24" fill="none" height="16" width="16"><path fill="#000" fill-rule="evenodd" clip-rule="evenodd" d="M8.2 2.2c1-.4 2 .6 1.6 1.5-1 3-.4 6.4 1.8 8.7a8.4 8.4.0 008.7 1.8c1-.3 2 .5 1.5 1.5v.1A10.3 10.3.0 0112.4 22 10.3 10.3.0 013.2 6.7c1-2 2.9-3.5 4.9-4.4z"/></svg></span></label></div><script>var darkMode=!1,themeSwitch;window.matchMedia("(prefers-color-scheme: dark)").matches&&(darkMode=!0),localStorage.getItem("theme")==="dark"?darkMode=!0:localStorage.getItem("theme")==="light"&&(darkMode=!1),darkMode&&document.documentElement.classList.toggle("dark"),themeSwitch=document.querySelectorAll("[data-theme-switcher]"),document.addEventListener("DOMContentLoaded",()=>{[].forEach.call(themeSwitch,function(e){e.checked=!!darkMode,e.addEventListener("click",()=>{document.documentElement.classList.toggle("dark"),localStorage.setItem("theme",document.documentElement.classList.contains("dark")?"dark":"light")})})})</script></div></nav></header><div class=search-modal aria-hidden=true style=--color-primary:#121212><div data-target=close-search-modal class=search-modal-overlay></div><div class=search-wrapper data-image=true data-description=true data-tags=true data-categories=true><div class=search-wrapper-header><label for=search-modal-input style=margin-top:-1px><span class=sr-only>search icon</span>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" height="18" width="18" class="search-icon" data-type="search"><path fill="currentcolor" d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" height="18" width="18" class="search-reset" data-type="reset"><path fill="currentcolor" d="M256 512A256 256 0 10256 0a256 256 0 100 512zM175 175c9.4-9.4 24.6-9.4 33.9.0l47 47 47-47c9.4-9.4 24.6-9.4 33.9.0s9.4 24.6.0 33.9l-47 47 47 47c9.4 9.4 9.4 24.6.0 33.9s-24.6 9.4-33.9.0l-47-47-47 47c-9.4 9.4-24.6 9.4-33.9.0s-9.4-24.6.0-33.9l47-47-47-47c-9.4-9.4-9.4-24.6.0-33.9z"/></svg>
</label><input id=search-modal-input type=text data-search-input autocomplete=off aria-label=Search placeholder=搜索></div><div class=search-wrapper-body><div class=search-result data-search-result></div><span class=search-result-empty>输入内容以搜索</span></div><div class=search-wrapper-footer><span><kbd><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="currentcolor" viewBox="0 0 16 16"><path d="M3.204 11h9.592L8 5.519 3.204 11zm-.753-.659 4.796-5.48a1 1 0 011.506.0l4.796 5.48c.566.647.106 1.659-.753 1.659H3.204a1 1 0 01-.753-1.659z"/></svg>
</kbd><kbd><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="currentcolor" style="margin-top:1px" viewBox="0 0 16 16"><path d="M3.204 5h9.592L8 10.481 3.204 5zm-.753.659 4.796 5.48a1 1 0 001.506.0l4.796-5.48c.566-.647.106-1.659-.753-1.659H3.204a1 1 0 00-.753 1.659z"/></svg>
</kbd>导航
</span><span><kbd><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentcolor" style="display:inline-block" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M14.5 1.5a.5.5.0 01.5.5v4.8a2.5 2.5.0 01-2.5 2.5H2.707l3.347 3.346a.5.5.0 01-.708.708l-4.2-4.2a.5.5.0 010-.708l4-4a.5.5.0 11.708.708L2.707 8.3H12.5A1.5 1.5.0 0014 6.8V2a.5.5.0 01.5-.5z"/></svg>
</kbd>选择
</span><span class=search-result-info></span>
<span data-target=close-search-modal><kbd>ESC</kbd> 关闭</span></div></div></div><main><section class="section pt-7"><div class=container><div class="row justify-center"><article class=lg:col-10><div class=mb-10><picture><source srcset=/images/bert/image-2_hu87100aa94ecaa21de31e7ce641853097_114162_545x0_resize_q80_h2_lanczos_3.webp media="(max-width: 575px)"><source srcset=/images/bert/image-2_hu87100aa94ecaa21de31e7ce641853097_114162_600x0_resize_q80_h2_lanczos_3.webp media="(max-width: 767px)"><source srcset=/images/bert/image-2_hu87100aa94ecaa21de31e7ce641853097_114162_700x0_resize_q80_h2_lanczos_3.webp media="(max-width: 991px)"><source srcset=/images/bert/image-2_hu87100aa94ecaa21de31e7ce641853097_114162_1110x0_resize_q80_h2_lanczos_3.webp><img loading=lazy decoding=async src=/images/bert/image-2_hu87100aa94ecaa21de31e7ce641853097_114162_1110x0_resize_lanczos_3.png class="w-full rounded img" alt=【学习笔记】BERT的基本结构与原理 width=1080 height=606></picture></div><h1 class="h2 mb-4">【学习笔记】BERT的基本结构与原理</h1><ul class=mb-4><li class="mr-4 inline-block"><a href=/en/authors/liang-hangfeng/><i class="fa-regular fa-circle-user mr-2"></i>Liang Hangfeng</a></li><li class="mr-4 inline-block"><i class="fa-regular fa-folder mr-2"></i>
<a href=/en/categories/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0/>深度学习
,
</a><a href=/en/categories/%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/>学习笔记</a></li><li class="mr-4 inline-block"><i class="fa-regular fa-clock mr-2"></i>
March 14, 2024</li></ul><div class="content mb-10"><h3 id=学习笔记transformer--bert>【学习笔记】Transformer & BERT</h3><h4 id=1-transformer>1. Transformer</h4><p>完全基于注意力机制（Attention）的模型。</p><p>考虑RNN处理文本问题，当输入的句子长度较长时，RNN很可能会遗忘之前句子中出现的词或者内容，Transformer中的Attention就使得句子中重要的内容的权重增大，以保证不会被遗忘，同时可以并行计算。</p><h5 id=encoder的网络结构>Encoder的网络结构</h5><p><img alt=image-1 src=/images/bert/image-1.png></p><p>Transformer为Encoder-Decoder结构，首先关注Encoder。</p><p>内部结构为：输入经过Input Embedding模块进行向量化，并且直接加上位置编码Positional Encoding，随后数据依次进入Muti-Head Attention模块，Add&amp;Norm模块，Feed Forward模块，另一个Add&amp;Norm模块，整个结构重复N次。</p><h5 id=positional-encoding>Positional Encoding</h5><p>来表示输入句子向量中的每个词的所在位置，使Transformer能够获得句子的时序信息。常见的计算方法就是使用正弦函数和余弦函数来构造每个位置的值，或者使用一些可训练的参数。</p><h5 id=self-attention>Self-Attention</h5><p>对于注意力机制，可以简单理解为对于任意一个需求的查询$Q$，都会面临各种要素的键值对$K,V$，通过匹配$Q$和$K$来获取指定的$V$的整合来作为该查询$Q$所能得到的反馈。相当于不同的查询$Q$具有不同的注意力点，得到的$V$也是不同的。例如将中文翻译成英文的过程，对于每一个中文单词，都有一个到英文句子的映射，对于英文句子中的某些单词具有更强的注意力。</p><p>简易的表达式如下，即根据$K$和$Q$的相似度加权取$V$进行组合。</p><p>$$
att((K,V),Q)=\sum_{i=1}^{N}\frac{exp(\mathbf{k_i}\mathbf{q})}{\sum_{j}exp(\mathbf{k_j}\mathbf{q})}v_i
$$</p><p>考虑到查询一般是一个词，词向量可能和$QKV$的维度不一样，因此考虑设置线性变换矩阵，将词向量线性变换到与$QKV$相同的维度。使用矩阵形式进行描述，首先计算$Q$和$K$的内积，并处以$\sqrt{d_k}$以防止内积过大，并通过softmax进行归一化。</p><p>$$
att(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$</p><p>而自注意力机制则考虑的是同一序列中不同位置的关联程度，它只关注输入本身或者对象本身，可以筛选全局中的重要信息，并且建立全局中的依赖关系。例如一个句子中的某一个单词，会聚焦于这个句子其它位置的单词。（自注意力的$Q$和$K$是相同来源的）</p><h5 id=multi-head-attention>Multi-Head Attention</h5><p>考虑：一段文字可能有多个维度的注意力，例如情感、时间、逻辑，则需要从多个不同的维度抓住输入信息的特点，因此有了多头注意力Multi-Head Attention，把输入序列映射为多组不同的$QKV$，分别进行计算后将结果进行合并(concat)。</p><p><img alt=image-2 src=/images/bert/image-2.png></p><p>在代码实现中，与上图的过程有所不同，只用了一个大一点的变换矩阵得到$QKV$，然后在特征维度上进行划分，得到h组$QKV$，对每一组$QKV$计算一个结果后进行concat。</p><p>BERT中的self-attention实现代码片段：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BERTSelfAttention</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#75715e># BERT 的 Self-Attention 类</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, config):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 初始化函数</span>
</span></span><span style=display:flex><span>        super(BERTSelfAttention, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        <span style=color:#75715e># 隐藏层(词向量)的维度需要是注意力头的整数倍，使得能够准确划分</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> config<span style=color:#f92672>.</span>hidden_size <span style=color:#f92672>%</span> config<span style=color:#f92672>.</span>num_attention_heads <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;The hidden size (</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>) is not a multiple of the number of attention &#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;heads (</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>)&#34;</span> <span style=color:#f92672>%</span> (config<span style=color:#f92672>.</span>hidden_size, config<span style=color:#f92672>.</span>num_attention_heads))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_attention_heads <span style=color:#f92672>=</span> config<span style=color:#f92672>.</span>num_attention_heads
</span></span><span style=display:flex><span>        <span style=color:#75715e># 每个注意力头的维数</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>attention_head_size <span style=color:#f92672>=</span> int(config<span style=color:#f92672>.</span>hidden_size <span style=color:#f92672>/</span> config<span style=color:#f92672>.</span>num_attention_heads)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>all_head_size <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>num_attention_heads <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>attention_head_size
</span></span><span style=display:flex><span>		<span style=color:#75715e># 将输入进行线性变换</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>query <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(config<span style=color:#f92672>.</span>hidden_size, self<span style=color:#f92672>.</span>all_head_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>key <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(config<span style=color:#f92672>.</span>hidden_size, self<span style=color:#f92672>.</span>all_head_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>value <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(config<span style=color:#f92672>.</span>hidden_size, self<span style=color:#f92672>.</span>all_head_size)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transpose_for_scores</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 调整维度，转换为 (batch_size, num_attention_heads, hidden_size, attention_head_size)</span>
</span></span><span style=display:flex><span>        new_x_shape <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>size()[:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> (self<span style=color:#f92672>.</span>num_attention_heads, self<span style=color:#f92672>.</span>attention_head_size)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>view(<span style=color:#f92672>*</span>new_x_shape)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, hidden_states):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 线性变换QKV</span>
</span></span><span style=display:flex><span>        mixed_query_layer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>query(hidden_states)
</span></span><span style=display:flex><span>        mixed_key_layer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>key(hidden_states)
</span></span><span style=display:flex><span>        mixed_value_layer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>value(hidden_states)
</span></span><span style=display:flex><span> 		<span style=color:#75715e># 维度调整</span>
</span></span><span style=display:flex><span>        query_layer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>transpose_for_scores(mixed_query_layer) 
</span></span><span style=display:flex><span>        key_layer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>transpose_for_scores(mixed_key_layer)
</span></span><span style=display:flex><span>        value_layer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>transpose_for_scores(mixed_value_layer)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将&#34;query&#34;和&#34;key&#34;点乘，得到未经处理注意力值</span>
</span></span><span style=display:flex><span>        attention_scores <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>matmul(query_layer, key_layer<span style=color:#f92672>.</span>transpose(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>        <span style=color:#75715e># 除以根号下的q的维度</span>
</span></span><span style=display:flex><span>        attention_scores <span style=color:#f92672>=</span> attention_scores <span style=color:#f92672>/</span> math<span style=color:#f92672>.</span>sqrt(self<span style=color:#f92672>.</span>attention_head_size)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 使用 softmax 函数将注意力值标准化成概率值</span>
</span></span><span style=display:flex><span>        attention_probs <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Softmax(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)(attention_scores)
</span></span><span style=display:flex><span> 		<span style=color:#75715e># 获取注意力分数</span>
</span></span><span style=display:flex><span>        context_layer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>matmul(attention_probs, value_layer)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 调整维度并返回</span>
</span></span><span style=display:flex><span>        context_layer <span style=color:#f92672>=</span> context_layer<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>)<span style=color:#f92672>.</span>contiguous()
</span></span><span style=display:flex><span>        new_context_layer_shape <span style=color:#f92672>=</span> context_layer<span style=color:#f92672>.</span>size()[:<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>] <span style=color:#f92672>+</span> (self<span style=color:#f92672>.</span>all_head_size,)
</span></span><span style=display:flex><span>        context_layer <span style=color:#f92672>=</span> context_layer<span style=color:#f92672>.</span>view(<span style=color:#f92672>*</span>new_context_layer_shape)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> context_layer
</span></span></code></pre></div><h5 id=add--norm>Add & Norm</h5><p>进行了残差连接和归一化，归一化选用的是LN。（单独分析词向量的某一维度是意义不大的，因此不选用BN对相同的维度进行归一化）</p><h5 id=feed-forward>Feed Forward</h5><p>全连接层的公式如下。先线性变换，然后ReLU非线性，再线性变换。经过FFN的维度并未发生变化，但本质上两层线性变换中，首先将输入映射到了一个更高的维度空间中，经过ReLU筛选后变回原来的维度。
$$
FFN(x)=max(0, xW_1+b_1)W_2+b_2
$$</p><h5 id=整体结构>整体结构</h5><p><img alt=image-3 src=/images/bert/image-3.png></p><h5 id=两种输入模式>两种输入模式</h5><p>Transformer在进行训练时，同时具有outputs的数据，而在预测时，将上一轮的输出作为输入（不能够知道当前时刻以后的信息）。</p><h5 id=masked-multi-head-attention>Masked Multi-Head Attention</h5><p>与多头注意力大体一致，增加了掩码，对某些值进行了覆盖，使其在参数更新时不产生效果。Transformer中涉及到两种Mask：padding mask和sequence mask。</p><p>对于padding mask：考虑到每次输入的序列长度是不一样的，需要对序列进行对齐操作（或者说是填充操作），这些填充的位置是没有意义的，即对于attention来说，不应该把注意力放在这些位置之上，因此可以在这些位置上加上很大的负数，那么经过softmax后，这些位置的概率会接近于0。</p><p>对于sequence mask，为了让decoder不能够看到未来的相关信息，在解码的时候只能依赖于当前时刻之前的输出，因此需要将当前时刻之后的信息进行隐藏。这部分信息在训练的时候是有效的，在训练的时候outputs的数据是完整地进入到decoder中的，但在预测时不需要。一种做法是产生一个上三角值全为0的上三角矩阵。</p><h4 id=2-bert>2. BERT</h4><p>来源论文《Pre-training of Deep Bidirectional Transformers for Language Understanding》，BERT（Bidirectional Encoder Representations from Transformers）。</p><h5 id=基本原理>基本原理</h5><p>BERT整体是一个自编码的语言模型，设计了两大任务来预训练该模型：1. 采用MaskLM的方法来训练，在输入一句话的时候，随机地选一些要预测的词，然后用一个特殊符号[MASK]来代替，然后让模型根据所给的标签去学习这些地方该填的词。 2. 在双向语言任务的基础上额外增加了一个句子级别的连续性预测任务，即预测输入BERT的两段文本是否为连续的文本，这个任务可以让模型学习到连续的文本片段之间的关系。</p><h5 id=bert与transformer>BERT与Transformer</h5><p>在BERT中只使用了Transformer的Encoder模块，原论文中作者使用了12层和24层Transformer Encoder组成的BERT模型。但与Transformer Encoder相比，BERT中的输入向量表示多了Segment Embeddings。</p><h5 id=masked-lm>Masked LM</h5><p>该任务即为：给定一个句子，随机抹去这句话的一些词，要求根据剩下的词去预测抹去的词是什么。相当于完形填空的任务。在文中，作者的做法为：随机选择15%的词用作预测，对于在原句中被抹去的词汇，80%用[MASK]替换，10%采用任意词替换，10%情况保持原词不变。这会使得在后续的微调任务中语句不会出现[MASK]标记，同时在预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（这有10%的概率），故使得模型更多依赖上下文信息去预测词汇，也赋予了模型纠错能力。</p><h5 id=next-sentence-prediction>Next Sentence Prediction</h5><p>该任务即为：给定一篇文章中的两句话，判断第二句话中文本是否紧跟在第一句话之后。衍生的任务可以是段落重排序任务。</p><p>联合这两个任务，使得模型输出的每一个词都能够尽可能全面地刻画输入文本的整体信息，为后续的微调任务提供更好的模型参数初始值。</p><h5 id=输入输出>输入输出</h5><p>BERT的输入为文本中各词的原始词向量（可以是随机的，也可以是经过Word2Ver算法进行预训练的结果）。模型的输出是文本中token经过全文语义信息的融合后的向量表示。如图。</p><p><img alt=image-4 src=/images/bert/image-4.png></p><p>模型的输出除了字向量（Token Embedding）外，还包含文本向量（Segment Embeddings）和位置向量（Positional Embeddings）。</p><p>文本向量：取值在模型的训练过程中自动学习，用于刻画文本的全局语义信息，并融合了单字的语义信息。</p><p>位置向量：刻画Token的出现位置。</p><p>实际上，在做 Next Sentence Prediction 任务时，在第一个句子的首部会加上一个[CLS] token，在两个句子中间以及最后一个句子的尾部会加上一个[SEP] token。</p><h4 id=3-代码以文本分类为例>3. 代码（以文本分类为例）</h4><h5 id=berttokenizer>BertTokenizer</h5><p>BERT分词器，对原句进行分词操作。</p><p>使用from_pretrained()函数传入预训练的模型参数。</p><p>**tokenize()**函数实现分词，中文默认会划分为一个个字，英文会按照空格进行划分。 同时会按照概率随机地进行Mask替换，也可以自行输入标记。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>tk <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;你的 我的 大家的&#39;</span>
</span></span><span style=display:flex><span>print(tk<span style=color:#f92672>.</span>tokenize(text))
</span></span><span style=display:flex><span><span style=color:#75715e># [&#39;[UNK]&#39;, &#39;的&#39;, &#39;我&#39;, &#39;的&#39;, &#39;大&#39;, &#39;家&#39;, &#39;的&#39;]</span>
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;Hello BERT for me&#39;</span>
</span></span><span style=display:flex><span>print(tk<span style=color:#f92672>.</span>tokenize(text))
</span></span><span style=display:flex><span><span style=color:#75715e># [&#39;hello&#39;, &#39;bert&#39;, &#39;for&#39;, &#39;me&#39;]</span>
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;Hello [MASK] BERT for me&#39;</span>
</span></span><span style=display:flex><span>print(tk<span style=color:#f92672>.</span>tokenize(text))
</span></span><span style=display:flex><span><span style=color:#75715e># [&#39;hello&#39;, &#39;[MASK]&#39;, &#39;bert&#39;, &#39;for&#39;, &#39;me&#39;]</span>
</span></span></code></pre></div><p><strong>convert_tokens_to_ids()</strong> 函数能够将字映射到id上，通常配合分词器使用。convert_ids_to_tokens,将id转化成token，通常用于模型预测出结果，查看时使用。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>tk <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;Hello [MASK] BERT for me&#39;</span>
</span></span><span style=display:flex><span>tokenized_text <span style=color:#f92672>=</span> tk<span style=color:#f92672>.</span>tokenize(text)
</span></span><span style=display:flex><span>ids <span style=color:#f92672>=</span> [tk<span style=color:#f92672>.</span>convert_tokens_to_ids(i) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> tokenized_text]
</span></span><span style=display:flex><span>print(ids)
</span></span><span style=display:flex><span><span style=color:#75715e># [7592, 103, 14324, 2005, 2033]</span>
</span></span></code></pre></div><p><strong>encode()</strong> 函数能够进行分词和token转换，相当于结合上述两个函数。输出结果也自动添加了开始标记和结束标记。特别地，encode函数可以传入列表，相当于可以手动地先进行分词，再传入进行token转换。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>tk <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;Hello [MASK] BERT for me&#39;</span>
</span></span><span style=display:flex><span>tokenized_text <span style=color:#f92672>=</span> tk<span style=color:#f92672>.</span>encode(text)
</span></span><span style=display:flex><span>print(tokenized_text)
</span></span><span style=display:flex><span><span style=color:#75715e># [101, 7592, 103, 14324, 2005, 2033, 102]</span>
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;今天的天气，有点差&#39;</span>
</span></span><span style=display:flex><span>tokenized_text <span style=color:#f92672>=</span> tk<span style=color:#f92672>.</span>encode(text)
</span></span><span style=display:flex><span>print(tokenized_text)
</span></span><span style=display:flex><span><span style=color:#75715e># [101, 100, 1811, 1916, 1811, 100, 1989, 1873, 100, 100, 102]</span>
</span></span></code></pre></div><p><strong>encode_plus()</strong> 函数可以在encode的基础之上生成input_ids、token_type_ids、attention_mask。它可以传入'123 123 123&rsquo;之类的数据，会被当成英文单词进行按空格划分。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>tk <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;今天的天气，有点差&#39;</span>
</span></span><span style=display:flex><span>tokenized_text <span style=color:#f92672>=</span> tk<span style=color:#f92672>.</span>encode_plus(text)
</span></span><span style=display:flex><span>print(tokenized_text)
</span></span><span style=display:flex><span><span style=color:#75715e># {&#39;input_ids&#39;: [101, 100, 1811, 1916, 1811, 100, 1989, 1873, 100, 100, 102], </span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], </span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</span>
</span></span></code></pre></div><p><strong>batch_encode_plus()</strong> 函数可以在encode_plus()函数的基础上进行词的批量处理。这会导致如果直接传入的是一个字符串，该函数会把它视作一个列表，因此可能需要套入一个列表。（该函数是从传入的列表依次取元素进行分词操作）</p><p>常用参数有add_special_tokens=True, max_length=128, pad_to_max_length=True。表示是否添加特殊标记，输出的词向量维度，是否进行补零直到max_length长度。truncation=True为大于max_length后截断。</p><p>处理好的input_ids和attention_mask的shape都是[batch, len]。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>tk <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;123 4&#39;</span>
</span></span><span style=display:flex><span>tokenized_text <span style=color:#f92672>=</span> tk<span style=color:#f92672>.</span>batch_encode_plus(text)
</span></span><span style=display:flex><span>print(tokenized_text)
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;input_ids&#39;: [[101, 1015, 102], [101, 1016, 102], [101, 1017, 102], [101, 102], [101, 1018, 102]]</span>
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;123 4&#39;</span>]
</span></span><span style=display:flex><span>tokenized_text <span style=color:#f92672>=</span> tk<span style=color:#f92672>.</span>batch_encode_plus(text)
</span></span><span style=display:flex><span>print(tokenized_text)
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;input_ids&#39;: [[101, 13138, 1018, 102]]</span>
</span></span></code></pre></div><h5 id=bertmodel>BertModel</h5><p>主要为Transformer的Encoder结构。</p><p>输入：</p><p>nput_ids：经过 tokenizer 分词后的 subword 对应的下标列表；attention_mask：在 self-attention 过程中，这一块 mask 用于标记 subword 所处句子和 padding 的区别，将 padding 部分填充为 0；</p><p>输出：</p><p>last_hidden_state[batch_size, sequence_length, hidden_size]，对每一个词都有一个长度为hidden_size的词向量表示。</p><p>pooler_output[batch_size, hidden_size]，这是序列的第一个token(classification token)的最后一层的隐藏状态，它是由线性层和Tanh激活函数进一步处理的。</p><h5 id=使用bert进行文本分类>使用BERT进行文本分类</h5><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BertClassification</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(BertClassification, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;model&#39;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> BertModel<span style=color:#f92672>.</span>from_pretrained(self<span style=color:#f92672>.</span>model_name)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(self<span style=color:#f92672>.</span>model_name)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>768</span>, <span style=color:#ae81ff>15</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        print(x)
</span></span><span style=display:flex><span>        batch_tokenized <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>.</span>batch_encode_plus(x, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, pad_to_max_length<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># input_ids[batch, len]  attention_mask[batch, len]</span>
</span></span><span style=display:flex><span>        input_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(batch_tokenized[<span style=color:#e6db74>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>        attention_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(batch_tokenized[<span style=color:#e6db74>&#39;attention_mask&#39;</span>])
</span></span><span style=display:flex><span>        <span style=color:#75715e># last_hidden_state[batch, len, hidden_len]</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># pooler_output</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># hidden_states 可选</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># attentions 可选</span>
</span></span><span style=display:flex><span>        hidden_outputs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model(input_ids,attention_mask<span style=color:#f92672>=</span>attention_mask)
</span></span><span style=display:flex><span>        <span style=color:#75715e># hidden_outputs[0][:,0,:].shape = [len, hidden_len] 取到CLS的Embedding</span>
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc(hidden_outputs[<span style=color:#ae81ff>0</span>][:,<span style=color:#ae81ff>0</span>,:])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> output
</span></span></code></pre></div><h5 id=句子表征>句子表征</h5><p>分类任务中，需要得到一个句子的Embedding，常见的方法有：</p><ol><li>直接用CLS Token的Embedding作为句子表征（其实也就是pooler_output的内容）但有缺点在于压缩的信息太多了</li><li>最后几层上下文 Embedding 的平均值：会损失语义，尤其是对Self-Attention取平均，甚至会弱化本来得到的Attention。</li><li>最后一层的Embedding再走一层CNN或者别的深度网络，但会增加一些学习的成本。</li></ol><h4 id=参考文章>参考文章</h4><p><a href=https://blog.csdn.net/weixin_44799217/article/details/115374101 target=_blank>BERT模型的详细介绍-CSDN博客</a></p><p><a href=https://blog.csdn.net/weixin_46713695/article/details/134189984 target=_blank>12-NLP之Bert实现文本分类_bert文本分类-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/539805336?utm_id=0" target=_blank>BERT_Tokenizer - 知乎 (zhihu.com)</a></p><p><a href=https://blog.csdn.net/weixin_48030475/article/details/128688629 target=_blank>【transformers】tokenizer用法（encode、encode_plus、batch_encode_plus等等）_transformers tokenizer参数-CSDN博客</a></p><p><a href=https://blog.csdn.net/pearl8899/article/details/116354207 target=_blank>Bert系列：如何用bert模型输出文本的embedding_bert获取embedding-CSDN博客</a></p><script type=text/javascript async src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style></div><div class="row items-start justify-between"><div class="lg:col-5 mb-10 flex items-center lg:mb-0"><h5 class=mr-3>标签 :</h5><ul><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/en/tags/%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86/>自然语言处理</a></li><li class=inline-block><a class="bg-theme-light hover:bg-primary dark:bg-darkmode-theme-light dark:hover:bg-darkmode-primary dark:hover:text-dark m-1 block rounded px-3 py-1 hover:text-white" href=/en/tags/%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb/>文本分类</a></li></ul></div></div></article></div></div></section></main><footer class="bg-theme-light dark:bg-darkmode-theme-light"><div class=container><div class="row items-center py-10"><div class="lg:col-3 mb-8 text-center lg:mb-0 lg:text-left"><a class="navbar-brand inline-block" href=/en/><img fetchpriority=high decoding=async class="img logo-light" width=160 height=32 src=/images/logo-1_hubf409f6fc77577ff09f7cacc73653041_8513_320x0_resize_q80_h2_lanczos_3.webp alt=Hugoplate onerror='this.onerror=null,this.src="/images/logo-1_hubf409f6fc77577ff09f7cacc73653041_8513_320x0_resize_lanczos_3.png"'>
<img fetchpriority=high decoding=async class="img logo-dark" width=160 height=32 src=/images/logo-2_hu06a606f114baa5722e40403b02652855_7961_320x0_resize_q80_h2_lanczos_3.webp alt=Hugoplate onerror='this.onerror=null,this.src="/images/logo-2_hu06a606f114baa5722e40403b02652855_7961_320x0_resize_lanczos_3.png"'></a></div><div class="lg:col-6 mb-8 text-center lg:mb-0"><ul><li class="m-3 inline-block"><a href=/en/about/>个人信息</a></li><li class="m-3 inline-block"><a href=/en/blog/>学习笔记</a></li><li class="m-3 inline-block"><a href=/en/privacy-policy/>相关说明</a></li></ul></div><div class="lg:col-3 mb-8 text-center lg:mb-0 lg:mt-0 lg:text-right"><ul class=social-icons><li><a target=_blank aria-label=github rel="nofollow noopener" href=https://www.github.com/COOOIKX><i class="fab fa-github"></i></a></li><li><a target=_blank aria-label=csdn rel="nofollow noopener" href="https://blog.csdn.net/m0_59701064?spm=1000.2115.3001.5343"><i class="fas fa-home-lg"></i></a></li></ul></div></div></div><div class="border-border dark:border-darkmode-border border-t py-7"><div class="text-light dark:text-darkmode-light container text-center"><p>Designed by Zeon Studio and Developed by LHF</p></div></div></footer><script crossorigin=anonymous integrity="sha256-YQerunHGeT7hXzxweSqFUXgOHHxFceSjmMy/kmnAHWU=" src=/js/script.min.6107abba71c6793ee15f3c70792a8551780e1c7c4571e4a398ccbf9269c01d65.js></script><script defer async crossorigin=anonymous integrity="sha256-w+aS42D2+B+Jix+joZ7pAua1vbu/pRK/IhoP55b8n3w=" src=/js/script-lazy.min.c3e692e360f6f81f898b1fa3a19ee902e6b5bdbbbfa512bf221a0fe796fc9f7c.js></script><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/service-worker.js")</script></body></html>